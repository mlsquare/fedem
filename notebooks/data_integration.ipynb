{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli login --token $TOKEN$ --add-to-git-credential "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import Dataset, load_dataset\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "dataset = load_dataset(\"mlsquare/samantar_merged_with_train_val\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/byt5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA COMBINATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict  # For creating a dictionary with default values\n",
    "\n",
    "# Import necessary libraries\n",
    "from datasets import (  # For loading datasets and concatenating them\n",
    "    DatasetDict,\n",
    "    concatenate_datasets,\n",
    "    load_dataset,\n",
    ")\n",
    "from tqdm import tqdm  # For progress bar visualization\n",
    "\n",
    "# List of language codes for datasets\n",
    "language_codes = [\"as\", \"bn\", \"gu\", \"hi\", \"kn\", \"ml\", \"mr\", \"or\", \"pa\", \"ta\", \"te\"]\n",
    "\n",
    "# Empty list to store datasets\n",
    "datasets_list = []\n",
    "\n",
    "# Loop through each language code\n",
    "for code in tqdm(language_codes):\n",
    "    # Load dataset for each language code\n",
    "    dataset = load_dataset(\"ai4bharat/samanantar\", code)\n",
    "    # Append only the training portion of the dataset to the list\n",
    "    datasets_list.append(dataset[\"train\"])\n",
    "\n",
    "# Concatenate all the datasets into a single dataset\n",
    "merged_dataset = concatenate_datasets(datasets_list)\n",
    "\n",
    "# Function to filter concatenated dataset based on a limit per language\n",
    "\n",
    "\n",
    "def filter_concatenated_dataset(merged_dataset, limit_per_language=5000):\n",
    "    # Dictionary to store filtered data\n",
    "    filtered_dict = defaultdict(list)\n",
    "    total = 0  # Counter for total number of samples processed\n",
    "\n",
    "    # Loop through each sample in the merged dataset\n",
    "    for sample in tqdm(merged_dataset):\n",
    "        # Check if the total number of samples processed is less than the limit per language times the number of languages\n",
    "        if total < limit_per_language * len(language_codes):\n",
    "            # Append the target and source texts to the filtered dictionary\n",
    "            filtered_dict[\"tgt\"].append(sample[\"tgt\"])\n",
    "            filtered_dict[\"src\"].append(sample[\"src\"])\n",
    "            total += 1  # Increment the counter\n",
    "        else:\n",
    "            break  # If the limit is reached, exit the loop\n",
    "\n",
    "    # Calculate the percentage of data retained after filtering\n",
    "    print(f\"{len(filtered_dict['tgt'])/total:.2%} of data after filtering.\")\n",
    "\n",
    "    # Return the filtered dataset as a Dataset object\n",
    "    return Dataset.from_dict({\"tgt\": filtered_dict[\"tgt\"], \"src\": filtered_dict[\"src\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_merged_dataset = filter_concatenated_dataset(\n",
    "    merged_dataset, limit_per_language=10000000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the number of samples for train and validation sets\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "train_size = int(len(filtered_merged_dataset) * 0.8)  # 80% for training\n",
    "valid_size = len(filtered_merged_dataset) - train_size  # Remaining for validation\n",
    "\n",
    "# Split the dataset into train and validation sets\n",
    "ds_train = filtered_merged_dataset.select(list(range(train_size)))\n",
    "ds_valid = filtered_merged_dataset.select(\n",
    "    list(range(train_size, train_size + valid_size))\n",
    ")\n",
    "\n",
    "# Create DatasetDict with train and validation sets\n",
    "raw_datasets = DatasetDict({\"train\": ds_train, \"valid\": ds_valid})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets.push_to_hub(\"mlsquare/samantar_merged_with_train_val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pose",
   "language": "python",
   "name": "pose"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
