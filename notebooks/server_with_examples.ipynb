{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from configuration_mamba import MambaConfig\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "from huggingface_hub import HfApi, ModelFilter\n",
    "from modeling_mamba import MambaForCausalLM, MambaModel\n",
    "from peft import LoraConfig, PeftMixedModel, TaskType, get_peft_model\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi, ModelFilter\n",
    "from peft import PeftMixedModel\n",
    "\n",
    "\n",
    "def get_models_by_organization(org_id):\n",
    "    api = HfApi()\n",
    "    new_filter = ModelFilter(tags=\"mamba\")\n",
    "    models = api.list_models(filter=new_filter)\n",
    "    models_list = []\n",
    "    for i in models:\n",
    "        print(i.modelId)\n",
    "        if org_id in i.modelId:\n",
    "            models_list.append(i.modelId)\n",
    "    return models_list\n",
    "\n",
    "\n",
    "org_id = \"mlsquare\"\n",
    "models = get_models_by_organization(org_id)\n",
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "adapters = {\n",
    "    \"small\": [\n",
    "        \"mlsquare/mamba_130M_small_out_proj\",\n",
    "        \"mlsquare/mamba_130M_small_dt_proj\",\n",
    "        \"mlsquare/mamba_130M_small_x_proj\",\n",
    "    ],\n",
    "    \"large\": [\"mlsquare/mamba_130M_large_x_dt_out_proj\"],\n",
    "}\n",
    "\n",
    "\n",
    "def compute_loss(model, inputs, return_outputs=False):\n",
    "    lm_logits = model(inputs)[0]\n",
    "    labels = inputs.to(lm_logits.device)\n",
    "\n",
    "    shift_logits = lm_logits[:, :-1, :].contiguous()\n",
    "    labels = labels[:, 1:].contiguous()\n",
    "    loss_fct = torch.nn.CrossEntropyLoss()\n",
    "    lm_loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), labels.view(-1))\n",
    "    return lm_loss\n",
    "\n",
    "\n",
    "def evaluation(data, model, tokenizer):\n",
    "    val = 0\n",
    "    for i in tqdm(data, desc=\"Evaluating\"):\n",
    "        value = tokenizer.encode(i['tgt'], return_tensors=\"pt\")\n",
    "        val += compute_loss(model, value)\n",
    "\n",
    "    avg_loss = val / len(data)\n",
    "    print(\"LOSS: \", avg_loss)\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "def model_merge_large(adapters, model_path, data, tokenizer):\n",
    "\n",
    "    model = MambaForCausalLM.from_pretrained(model_path)\n",
    "    print(\"model loaded\")\n",
    "\n",
    "    model.load_adapter(adapters[\"large\"][0])\n",
    "    print(\"adapter merged\")\n",
    "\n",
    "    result = evaluation(data, model, tokenizer)\n",
    "    return result\n",
    "\n",
    "\n",
    "def model_merge_small(adapters, model_path, data, tokenizer):\n",
    "\n",
    "    base_model = MambaForCausalLM.from_pretrained(\n",
    "        model_path, token=\"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\"\n",
    "    )\n",
    "    print(\"model loaded\")\n",
    "\n",
    "    peft_model = PeftMixedModel.from_pretrained(\n",
    "        base_model, adapters[\"small\"][0], token=\"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\"\n",
    "    )\n",
    "    peft_model.load_adapter(adapters[\"small\"][1], adapter_name=\"1\")\n",
    "    peft_model.load_adapter(adapters[\"small\"][2], adapter_name=\"2\")\n",
    "    peft_model.set_adapter([\"default\", \"1\", \"2\"])\n",
    "    print(\"adapter merged\")\n",
    "\n",
    "    result = evaluation(data, peft_model, tokenizer)\n",
    "    return result\n",
    "\n",
    "\n",
    "def create_JSON(value):\n",
    "    json_data = json.dumps(value, indent=4)\n",
    "    with open(f\"{value}\", \"w\") as json_file:\n",
    "        json_file.write(json_data)\n",
    "\n",
    "\n",
    "def get_data(data_path, fraction=0.01):\n",
    "    data = load_dataset(data_path)['train'].shuffle()\n",
    "    data = data.select(list(range(int(len(data) * fraction))))\n",
    "    print(\"data fetched\")\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_tokenizer(path):\n",
    "    return AutoTokenizer.from_pretrained(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_data(mamba_130M_small[\"data\"])\n",
    "tokenizer = load_tokenizer(mamba_130M_small[\"tokenizer_path\"])\n",
    "result = model_merge_small(adapters, mamba_130M_small[\"model_path\"], data, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_data(mamba_130M_large[\"data\"])\n",
    "tokenizer = load_tokenizer(mamba_130M_large[\"tokenizer_path\"])\n",
    "result = model_merge_small(adapters, mamba_130M_large[\"model_path\"], data, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "<model>-<PARAMS>-<AdapterComputation>-<target_modules>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mamba_130M_small = {\n",
    "    \"model_path\": \"mlsquare/pico_seshu\",\n",
    "    \"tokenizer_path\": \"google/byt5-large\",\n",
    "    \"adapter_path\": \"mlsquare/mamba_130M_large_x_dt\",\n",
    "    \"data\": \"mlsquare/samantar1per_cent_merged_with_train_val\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mamba_130M_large = {\n",
    "    \"model_path\": \"mlsquare/pico_seshu\",\n",
    "    \"tokenizer_path\": \"google/byt5-large\",\n",
    "    \"adapter_path\": \"mlsquare/mamba_130M_large_x_dt\",\n",
    "    \"data\": \"mlsquare/samantar1per_cent_merged_with_train_val\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fedem import server\n",
    "\n",
    "mamba_130M_small = {\n",
    "    \"model_path\": \"mlsquare/pico_seshu\",\n",
    "    \"tokenizer_path\": \"google/byt5-large\",\n",
    "    \"adapter_path\": \"mlsquare/mamba_130M_large_x_dt\",\n",
    "    \"data\": \"mlsquare/samantar1per_cent_merged_with_train_val\",\n",
    "}\n",
    "\n",
    "adapters = {\n",
    "    \"small\": [\n",
    "        \"mlsquare/mamba_130M_small_out_proj\",\n",
    "        \"mlsquare/mamba_130M_small_dt_proj\",\n",
    "        \"mlsquare/mamba_130M_small_x_proj\",\n",
    "    ],\n",
    "    \"large\": [\"mlsquare/mamba_130M_large_x_dt_out_proj\"],\n",
    "}\n",
    "\n",
    "data = server.get_data(mamba_130M_small[\"data\"])\n",
    "tokenizer = server.load_tokenizer(mamba_130M_small[\"tokenizer_path\"])\n",
    "result = server.model_merge_small(\n",
    "    adapters, mamba_130M_small[\"model_path\"], data, tokenizer\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "code",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
