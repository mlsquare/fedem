{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rp6vwkyHZZsj",
    "outputId": "47fe6983-8006-4fa3-cf50-0f36a1049d12"
   },
   "outputs": [],
   "source": [
    "pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LebeBe2JZOz2",
    "outputId": "3a77e37e-3fb1-4d96-9ef3-8520c8f54e24"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import urllib.request\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from einops import rearrange\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ms4PbkJHZQL9"
   },
   "outputs": [],
   "source": [
    "# Configuration flags and hyperparameters\n",
    "USE_MAMBA = 1\n",
    "DIFFERENT_H_STATES_RECURRENT_UPDATE_MECHANISM = 0\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rc9n8A_kZVZG"
   },
   "outputs": [],
   "source": [
    "d_model = 8\n",
    "state_size = 128  # Example state size\n",
    "seq_len = 100  # Example sequence length\n",
    "batch_size = 256  # Example batch size\n",
    "last_batch_size = 81  # only for the very last batch of the dataset\n",
    "current_batch_size = batch_size\n",
    "different_batch_size = False\n",
    "h_new = None\n",
    "temp_buffer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eqa7vWfvZfIO"
   },
   "outputs": [],
   "source": [
    "class S6(nn.Module):\n",
    "    def __init__(self, seq_len, d_model, state_size, device):\n",
    "        super(S6, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(d_model, d_model, device=device)\n",
    "        self.fc2 = nn.Linear(d_model, state_size, device=device)\n",
    "        self.fc3 = nn.Linear(d_model, state_size, device=device)\n",
    "\n",
    "        self.seq_len = seq_len\n",
    "        self.d_model = d_model\n",
    "        self.state_size = state_size\n",
    "\n",
    "        # self.A = nn.Parameter(torch.ones(d_model, state_size, device=device))\n",
    "        self.A = nn.Parameter(\n",
    "            F.normalize(torch.ones(d_model, state_size, device=device), p=2, dim=-1)\n",
    "        )\n",
    "        nn.init.xavier_uniform_(self.A)\n",
    "\n",
    "        self.B = torch.zeros(batch_size, self.seq_len, self.state_size, device=device)\n",
    "        self.C = torch.zeros(batch_size, self.seq_len, self.state_size, device=device)\n",
    "\n",
    "        self.delta = torch.zeros(batch_size, self.seq_len, self.d_model, device=device)\n",
    "        self.dA = torch.zeros(\n",
    "            batch_size, self.seq_len, self.d_model, self.state_size, device=device\n",
    "        )\n",
    "        self.dB = torch.zeros(\n",
    "            batch_size, self.seq_len, self.d_model, self.state_size, device=device\n",
    "        )\n",
    "\n",
    "        # h should have dimensions [batch_size, seq_len, d_model, state_size]\n",
    "        self.h = torch.zeros(\n",
    "            batch_size, self.seq_len, self.d_model, self.state_size, device=device\n",
    "        )\n",
    "        self.y = torch.zeros(batch_size, self.seq_len, self.d_model, device=device)\n",
    "\n",
    "    def discretization(self):\n",
    "        # discretization function is defined based on the MAMBA paper's description using ZOH on page 28\n",
    "        # in Section C : Mechanics on Selective SSMs\n",
    "        # See also \"Zero-order hold discretization\" maths proof inside https://studywolf.wordpress.com/tag/zero-order-hold/\n",
    "        \"\"\"\n",
    "        Here is an explanation of the mathematical rationale for the formulation of Δt used in Mamba:\n",
    "        The key idea is that Δt controls the discretization rate of the continuous SSM dynamics. By making Δt input-dependent, it introduces selectivity into the discrete transition matrices.\n",
    "        Specifically, in Mamba they parameterize Δt as:\n",
    "        Δt = τΔ(Parameter + sΔ(xt))\n",
    "        Where:\n",
    "        - Parameter is a learned scalar parameter that controls the baseline discretization rate\n",
    "        - sΔ(xt) is a projection that makes Δt input-dependent by computing a value based on xt\n",
    "        - τΔ(x) = softplus(x) transforms the result to be positive through the softplus nonlinearity\n",
    "        The rationale for this formulation is:\n",
    "        - Parameter provides a reasonable default discretization rate\n",
    "        - sΔ(xt) injects input-dependence through the projection\n",
    "        - softplus ensures Δt is positive as required to be a valid timestep\n",
    "        - The projection sΔ allows the model to learn to modulate Δt based on the input xt\n",
    "        - This modulation creates selectivity in how rapidly or slowly the states update\n",
    "        So in summary, the learned input-dependent projection allows Δt, and thus the discrete dynamics, to become selective. The softplus and scalar parameter provide useful inductive biases on top of this flexibility.\n",
    "        The end result is discrete transition matrices that are selective on the input, enabling powerful sequence modeling capabilities.\n",
    "        Credit: Claude2 AI chatbot\n",
    "        \"\"\"\n",
    "\n",
    "        # inverse() only supports square matrix\n",
    "        # dB = torch.matmul(torch.inverse(A * delta), torch.matmul(dA - torch.eye(A.shape[0]), B))\n",
    "        self.dB = torch.einsum(\"bld,bln->bldn\", self.delta, self.B)\n",
    "\n",
    "        # https://github.com/state-spaces/mamba/blob/0131c1e94a46fc9f70bcfc9d57962963bb2f0b9e/mamba_ssm/modules/mamba_simple.py#L240\n",
    "        # dA = torch.matrix_exp(A * delta)  # matrix_exp() only supports square matrix\n",
    "        self.dA = torch.exp(torch.einsum(\"bld,dn->bldn\", self.delta, self.A))\n",
    "        # print(f\"self.dA.shape = {self.dA.shape}\")\n",
    "        # print(f\"self.dA.requires_grad = {self.dA.requires_grad}\")\n",
    "\n",
    "        return self.dA, self.dB\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Refer to Algorithm 2 in the MAMBA paper\n",
    "        self.B = self.fc2(x)\n",
    "        self.C = self.fc3(x)\n",
    "        self.delta = F.softplus(self.fc1(x))\n",
    "\n",
    "        # Uses ZOH as in MAMBA, Hungry Hippo still uses bilinear transform for discretization\n",
    "        self.discretization()\n",
    "\n",
    "        if (\n",
    "            DIFFERENT_H_STATES_RECURRENT_UPDATE_MECHANISM\n",
    "        ):  # this will trigger in-place runtime error if without using `h_new`\n",
    "\n",
    "            global current_batch_size\n",
    "            current_batch_size = x.shape[0]\n",
    "\n",
    "            if self.h.shape[0] != current_batch_size:\n",
    "                # print(\"Adjusting h_new for the different batch size of input data `x`\")\n",
    "                different_batch_size = True\n",
    "\n",
    "                # Resize self.h to match the current batch size\n",
    "                h_new = (\n",
    "                    torch.einsum(\n",
    "                        \"bldn,bldn->bldn\", self.dA, self.h[:current_batch_size, ...]\n",
    "                    )\n",
    "                    + rearrange(x, \"b l d -> b l d 1\") * self.dB\n",
    "                )\n",
    "\n",
    "            else:\n",
    "                different_batch_size = False\n",
    "                h_new = (\n",
    "                    torch.einsum(\"bldn,bldn->bldn\", self.dA, self.h)\n",
    "                    + rearrange(x, \"b l d -> b l d 1\") * self.dB\n",
    "                )\n",
    "\n",
    "            # y needs to have a shape of [batch_size, seq_len, d_model]\n",
    "            self.y = torch.einsum(\"bln,bldn->bld\", self.C, h_new)\n",
    "\n",
    "            # Update self.h with the detached state of h_new\n",
    "            # Only do this if retaining gradients for self.h is not necessary for backprop\n",
    "            # Otherwise, store h_new in a temporary list and update self.h after the loop\n",
    "            global temp_buffer\n",
    "            temp_buffer = (\n",
    "                h_new.detach().clone() if not self.h.requires_grad else h_new.clone()\n",
    "            )\n",
    "\n",
    "            return self.y\n",
    "\n",
    "        else:  # this will not trigger in-place runtime error\n",
    "            # h should have dimensions [batch_size, seq_len, d_model, state_size]\n",
    "            h = torch.zeros(\n",
    "                x.size(0), self.seq_len, self.d_model, self.state_size, device=x.device\n",
    "            )\n",
    "            y = torch.zeros_like(x)\n",
    "\n",
    "            h = (\n",
    "                torch.einsum(\"bldn,bldn->bldn\", self.dA, h)\n",
    "                + rearrange(x, \"b l d -> b l d 1\") * self.dB\n",
    "            )\n",
    "\n",
    "            # y needs to have a shape of [batch_size, seq_len, d_model]\n",
    "            y = torch.einsum(\"bln,bldn->bld\", self.C, h)\n",
    "\n",
    "            return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iJOD9GERZkSK"
   },
   "outputs": [],
   "source": [
    "class MambaBlock(nn.Module):\n",
    "    def __init__(self, seq_len, d_model, state_size, device):\n",
    "        super(MambaBlock, self).__init__()\n",
    "\n",
    "        self.inp_proj = nn.Linear(d_model, 2 * d_model, device=device)\n",
    "        self.out_proj = nn.Linear(2 * d_model, d_model, device=device)\n",
    "\n",
    "        # For residual skip connection\n",
    "        self.D = nn.Linear(d_model, 2 * d_model, device=device)\n",
    "\n",
    "        # Set _no_weight_decay attribute on bias\n",
    "        self.out_proj.bias._no_weight_decay = True\n",
    "\n",
    "        # Initialize bias to a small constant value\n",
    "        nn.init.constant_(self.out_proj.bias, 1.0)\n",
    "\n",
    "        self.S6 = S6(seq_len, 2 * d_model, state_size, device)\n",
    "\n",
    "        # Add 1D convolution with kernel size 3\n",
    "        self.conv = nn.Conv1d(seq_len, seq_len, kernel_size=3, padding=1, device=device)\n",
    "\n",
    "        # Add linear layer for conv output\n",
    "        self.conv_linear = nn.Linear(2 * d_model, 2 * d_model, device=device)\n",
    "\n",
    "        # rmsnorm\n",
    "        self.norm = RMSNorm(d_model, device=device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x_proj.shape = torch.Size([batch_size, seq_len, 2*d_model])\n",
    "        x_conv.shape = torch.Size([batch_size, seq_len, 2*d_model])\n",
    "        x_conv_act.shape = torch.Size([batch_size, seq_len, 2*d_model])\n",
    "        \"\"\"\n",
    "        # Refer to Figure 3 in the MAMBA paper\n",
    "\n",
    "        x = self.norm(x)\n",
    "\n",
    "        x_proj = self.inp_proj(x)\n",
    "        # print(f\"x_proj.shape = {x_proj.shape}\")\n",
    "\n",
    "        # Add 1D convolution with kernel size 3\n",
    "        x_conv = self.conv(x_proj)\n",
    "        # print(f\"x_conv.shape = {x_conv.shape}\")\n",
    "\n",
    "        x_conv_act = F.silu(x_conv)\n",
    "        # print(f\"x_conv_act.shape = {x_conv_act.shape}\")\n",
    "\n",
    "        # Add linear layer for conv output\n",
    "        x_conv_out = self.conv_linear(x_conv_act)\n",
    "        # print(f\"x_conv_out.shape = {x_conv_out.shape}\")\n",
    "\n",
    "        x_ssm = self.S6(x_conv_out)\n",
    "        x_act = F.silu(x_ssm)  # Swish activation can be implemented as x * sigmoid(x)\n",
    "        # print(f\"x_act.shape = {x_act.shape}\")\n",
    "\n",
    "        # residual skip connection with nonlinearity introduced by multiplication\n",
    "        x_residual = F.silu(self.D(x))\n",
    "        # print(f\"x_residual.shape = {x_residual.shape}\")\n",
    "        x_combined = x_act * x_residual\n",
    "        # print(f\"x_combined.shape = {x_combined.shape}\")\n",
    "\n",
    "        x_out = self.out_proj(x_combined)\n",
    "        # print(f\"x_out.shape = {x_out.shape}\")\n",
    "\n",
    "        return x_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QiqkaJY_Zn1s"
   },
   "outputs": [],
   "source": [
    "class Mamba(nn.Module):\n",
    "    def __init__(self, seq_len, d_model, state_size, device):\n",
    "        super(Mamba, self).__init__()\n",
    "        self.mamba_block1 = MambaBlock(seq_len, d_model, state_size, device)\n",
    "        self.mamba_block2 = MambaBlock(seq_len, d_model, state_size, device)\n",
    "        self.mamba_block3 = MambaBlock(seq_len, d_model, state_size, device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.mamba_block1(x)\n",
    "        x = self.mamba_block2(x)\n",
    "        x = self.mamba_block3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c0b0wF33ZqKL"
   },
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, d_model: int, eps: float = 1e-5, device: str = \"cuda\"):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(d_model, device=device))\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps) * self.weight\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fi_thF0yZtwy",
    "outputId": "6ea0c89e-cfa2-4fce-a0de-3787c4eabbff"
   },
   "outputs": [],
   "source": [
    "x = torch.rand(batch_size, seq_len, d_model, device=device)\n",
    "# Create the Mamba model\n",
    "mamba = Mamba(seq_len, d_model, state_size, device)\n",
    "\n",
    "# rmsnorm\n",
    "norm = RMSNorm(d_model)\n",
    "x = norm(x)\n",
    "\n",
    "# Forward pass\n",
    "test_output = mamba(x)\n",
    "print(\n",
    "    f\"test_output.shape = {test_output.shape}\"\n",
    ")  # Should be [batch_size, seq_len, d_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MlsYvgFlZwaW"
   },
   "outputs": [],
   "source": [
    "# Define a function for padding\n",
    "\n",
    "\n",
    "def pad_sequences_3d(sequences, max_len=None, pad_value=0):\n",
    "    # Assuming sequences is a tensor of shape (batch_size, seq_len, feature_size)\n",
    "    batch_size, seq_len, feature_size = sequences.shape\n",
    "\n",
    "    if max_len is None:\n",
    "        max_len = seq_len + 1\n",
    "\n",
    "    # Initialize padded_sequences with the pad_value\n",
    "    padded_sequences = torch.full(\n",
    "        (batch_size, max_len, feature_size),\n",
    "        fill_value=pad_value,\n",
    "        dtype=sequences.dtype,\n",
    "        device=sequences.device,\n",
    "    )\n",
    "    # Pad each sequence to the max_len\n",
    "    padded_sequences[:, :seq_len, :] = sequences\n",
    "\n",
    "    return padded_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GmzgVkYjaLGM"
   },
   "outputs": [],
   "source": [
    "def train(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    data_loader,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    device,\n",
    "    max_grad_norm=1.0,\n",
    "    DEBUGGING_IS_ON=False,\n",
    "):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_data = batch[\"input_ids\"].clone().to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].clone().to(device)\n",
    "\n",
    "        # In most sequence modeling tasks, like language modeling, the target should be the next token\n",
    "        # in the sequence rather than the input token itself.\n",
    "        # This is because the model's goal is to predict the next word given the previous words.\n",
    "        # Shift the input data by one position to get the target, so that each target token\n",
    "        # is the next token following the input token.\n",
    "        target = input_data[:, 1:]\n",
    "        input_data = input_data[:, :-1]\n",
    "\n",
    "        # Pad all the sequences in the batch:\n",
    "        input_data = pad_sequences_3d(input_data, pad_value=tokenizer.pad_token_id)\n",
    "        target = pad_sequences_3d(\n",
    "            target, max_len=input_data.size(1), pad_value=tokenizer.pad_token_id\n",
    "        )\n",
    "\n",
    "        if USE_MAMBA:\n",
    "            output = model(input_data)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "        loss.backward(retain_graph=True)\n",
    "\n",
    "        # Clip gradients: gradients are modified in place\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        for name, param in model.named_parameters():\n",
    "            if \"out_proj.bias\" not in name:\n",
    "                # clip weights but not bias for out_proj\n",
    "                torch.nn.utils.clip_grad_norm_(param, max_norm=max_grad_norm)\n",
    "\n",
    "        if DEBUGGING_IS_ON:\n",
    "            for name, parameter in model.named_parameters():\n",
    "                if parameter.grad is not None:\n",
    "                    print(f\"{name} gradient: {parameter.grad.data.norm(2)}\")\n",
    "                else:\n",
    "                    print(f\"{name} has no gradient\")\n",
    "\n",
    "        if USE_MAMBA and DIFFERENT_H_STATES_RECURRENT_UPDATE_MECHANISM:\n",
    "            model.S6.h[:current_batch_size, ...].copy_(temp_buffer)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "66L7_G4UbWAk"
   },
   "outputs": [],
   "source": [
    "class Enwiki8Dataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data[\"input_ids\"])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx].clone().detach() for key, val in self.data.items()}\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fHh3bBHbaOKl"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_data = batch[\"input_ids\"].clone().detach().to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].clone().detach().to(device)\n",
    "\n",
    "            # In most sequence modeling tasks, like language modeling, the target should be the next token\n",
    "            # in the sequence rather than the input token itself.\n",
    "            # This is because the model's goal is to predict the next word given the previous words.\n",
    "            # Shift the input data by one position to get the target, so that each target token\n",
    "            # is the next token following the input token.\n",
    "            target = input_data[:, 1:]\n",
    "            input_data = input_data[:, :-1]\n",
    "\n",
    "            # Pad all the sequences in the batch:\n",
    "            input_data = pad_sequences_3d(input_data, pad_value=tokenizer.pad_token_id)\n",
    "            target = pad_sequences_3d(\n",
    "                target, max_len=input_data.size(1), pad_value=tokenizer.pad_token_id\n",
    "            )\n",
    "\n",
    "            if USE_MAMBA:\n",
    "                output = model(input_data)\n",
    "                loss = criterion(output, target)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "emfuv7UBaTrL"
   },
   "outputs": [],
   "source": [
    "def calculate_perplexity(loss):\n",
    "    return math.exp(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-R7pMVkcaWIt"
   },
   "outputs": [],
   "source": [
    "def load_enwiki8_dataset():\n",
    "    print(f\"Download and extract enwiki8 data\")\n",
    "    url = \"http://mattmahoney.net/dc/enwik8.zip\"\n",
    "    urllib.request.urlretrieve(url, \"enwik8.zip\")\n",
    "\n",
    "    with ZipFile(\"enwik8.zip\") as f:\n",
    "        data = f.read(\"enwik8\").decode(\"utf-8\")\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MYmX-gdwaYLG"
   },
   "outputs": [],
   "source": [
    "# Tokenize and encode the dataset\n",
    "\n",
    "\n",
    "def encode_dataset(tokenizer, text_data):\n",
    "    def batch_encode(tokenizer, text_data, batch_size=1000):\n",
    "        batched_input_ids = []\n",
    "        for i in range(0, len(text_data), batch_size):\n",
    "            batch = text_data[i : i + batch_size]\n",
    "            inputs = tokenizer(\n",
    "                batch,\n",
    "                add_special_tokens=True,\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                max_length=seq_len,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            batched_input_ids.append(inputs[\"input_ids\"])\n",
    "        return torch.cat(batched_input_ids)\n",
    "\n",
    "    input_ids = batch_encode(tokenizer, enwiki8_data)\n",
    "\n",
    "    global vocab_size\n",
    "    vocab_size = tokenizer.vocab_size\n",
    "    print(f\"vocab_size = {vocab_size}\")\n",
    "\n",
    "    embedding_layer = nn.Embedding(num_embeddings=vocab_size, embedding_dim=d_model)\n",
    "\n",
    "    def batch_embedding_calls(input_ids, embedding_layer, batch_size=256):\n",
    "        if not isinstance(input_ids, torch.Tensor):\n",
    "            input_ids = torch.tensor(input_ids, dtype=torch.long)\n",
    "\n",
    "        # Calculate the number of batches needed\n",
    "        num_batches = math.ceil(input_ids.size(0) / batch_size)\n",
    "\n",
    "        # List to hold the output embeddings\n",
    "        output_embeddings = []\n",
    "\n",
    "        # Process each batch\n",
    "        for i in range(num_batches):\n",
    "            # Calculate start and end indices for the current batch\n",
    "            start_idx = i * batch_size\n",
    "            end_idx = start_idx + batch_size\n",
    "\n",
    "            # Get the batch\n",
    "            input_id_batch = input_ids[start_idx:end_idx]\n",
    "\n",
    "            # Call the embedding layer\n",
    "            with torch.no_grad():  # No need gradients for this operation\n",
    "                batch_embeddings = embedding_layer(input_id_batch)\n",
    "\n",
    "            # Append the result to the list\n",
    "            output_embeddings.append(batch_embeddings)\n",
    "\n",
    "        # Concatenate the embeddings from each batch into a single tensor\n",
    "        all_embeddings = torch.cat(output_embeddings, dim=0)\n",
    "\n",
    "        return all_embeddings\n",
    "\n",
    "    # `input_ids` is a list or tensor of the input IDs and `embedding_layer` is model's embedding layer\n",
    "    if USE_MAMBA:\n",
    "        # Set `batch_size` to a value that works for memory constraints\n",
    "        encoded_inputs = batch_embedding_calls(\n",
    "            input_ids, embedding_layer, batch_size=1\n",
    "        ).float()\n",
    "\n",
    "    attention_mask = (input_ids != tokenizer.pad_token_id).type(input_ids.dtype)\n",
    "\n",
    "    return encoded_inputs, attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mpWW-gMKabSX"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google/byt5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "apgoWPnJadLU",
    "outputId": "19700227-e3a7-4eab-93db-dba7b26e1bac"
   },
   "outputs": [],
   "source": [
    "# Assuming encoded_inputs is a preprocessed tensor of shape [num_samples, seq_len, d_model]\n",
    "encoded_inputs_file = \"encoded_inputs_mamba.pt\"\n",
    "\n",
    "\n",
    "if os.path.exists(encoded_inputs_file):\n",
    "    print(\"Loading pre-tokenized data...\")\n",
    "    encoded_inputs = torch.load(encoded_inputs_file)\n",
    "else:\n",
    "    print(\"Tokenizing raw data...\")\n",
    "    enwiki8_data = load_enwiki8_dataset()\n",
    "    encoded_inputs, attention_mask = encode_dataset(tokenizer, enwiki8_data)\n",
    "    torch.save(encoded_inputs, encoded_inputs_file)\n",
    "    print(f\"finished tokenizing data\")\n",
    "\n",
    "\n",
    "# Combine into a single dictionary\n",
    "data = {\"input_ids\": encoded_inputs, \"attention_mask\": attention_mask}\n",
    "\n",
    "# Split the data into train and validation sets\n",
    "total_size = len(data[\"input_ids\"])\n",
    "train_size = int(total_size * 0.8)\n",
    "\n",
    "train_data = {key: val[:train_size] for key, val in data.items()}\n",
    "val_data = {key: val[train_size:] for key, val in data.items()}\n",
    "\n",
    "train_dataset = Enwiki8Dataset(train_data)\n",
    "val_dataset = Enwiki8Dataset(val_data)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "\n",
    "model = Mamba(seq_len, d_model, state_size, device).to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-6)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 25  # Number of epochs to train for\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):  # loop over the dataset multiple times\n",
    "    train_loss = train(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        train_loader,\n",
    "        optimizer,\n",
    "        criterion,\n",
    "        device,\n",
    "        max_grad_norm=10.0,\n",
    "        DEBUGGING_IS_ON=False,\n",
    "    )\n",
    "    val_loss = evaluate(model, val_loader, criterion, device)\n",
    "    val_perplexity = calculate_perplexity(val_loss)\n",
    "    print(\n",
    "        f\"Epoch: {epoch+1}, Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, Validation Perplexity: {val_perplexity:.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YgKGSAMlag6a",
    "outputId": "abd8f96f-a91a-4d30-b968-7c3e9a11a5bf"
   },
   "outputs": [],
   "source": [
    "# from transformers import T5ForConditionalGeneration, AutoTokenizer\n",
    "\n",
    "# # model = T5ForConditionalGeneration.from_pretrained(\"google/byt5-small\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"google/byt5-small\")\n",
    "\n",
    "# model_inputs = tokenizer(\n",
    "#     [\"Life is like a box of chocolates.\", \"Today is Monday.\"], padding=\"longest\", return_tensors=\"pt\"\n",
    "# )\n",
    "\n",
    "# print(model_inputs['input_ids'].shape)\n",
    "# labels_dict = tokenizer(\n",
    "#     [\"La vie est comme une boîte de chocolat.\", \"Aujourd'hui c'est lundi.\"], padding=\"longest\", return_tensors=\"pt\"\n",
    "# )\n",
    "# # print(labels_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MB5QvfBKQ07c"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data_folder = \"/kaggle/input/samanantar/final_data/\"\n",
    "\n",
    "data_dict = {}\n",
    "\n",
    "for folder_name in os.listdir(data_folder):\n",
    "    folder_path = os.path.join(data_folder, folder_name)\n",
    "    if os.path.isdir(folder_path):\n",
    "        language_data = {}\n",
    "        for file_name in os.listdir(folder_path):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            if os.path.isfile(file_path):\n",
    "                with open(file_path, \"r\") as file:\n",
    "                    sentences = file.readlines()\n",
    "                    total_sentences = 500000\n",
    "                    sentences = sentences[:total_sentences]\n",
    "                    sentences = [sentence.rstrip(\"\\n\") for sentence in sentences]\n",
    "                    language_data[file_name] = sentences\n",
    "\n",
    "        data_dict[folder_name] = language_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mSZHR4FYL9k5"
   },
   "source": [
    "🐍"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
